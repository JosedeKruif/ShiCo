
A: Vocabulary space (size N, ~600K)
a: embbeded vector space (size n,300), representing the semantic meaning

B: Vocabulary space (size M, 600K)
b: embbeded vector space (size m, 300)

Word 2 vec is the model that maps the vocabulary on the semantic meaning space:
    F_1 (A) -> a 
with a matrix it has dimensions (n,N), with the representation `model.syn0norm`.
Because N > n, the projection is not invertible, but there is a pseudoinverse G such that:
    F_1( G_1(a) ) = a

This allows us to compare the semantic spaces of A and B:
1) take a unit vector ui, in a with ui = [0, 0,.. 1, .., 0, 0]
2) find a word vector W in A via the inverse G_1:
   W1 = G_1( ui )
   note that this one is not unique, as the pseudoinverse is not a real inverse:
   F( W1 + N ) = ui, with N an element in the kernel of F.
   An interpretation is that as F(N) = 0 by definition, this vector N is meaning less wrt. the semantic subspace.
3) As W1 is a vector over the vocabulary A, and vocabulary B could be different, map W1 to W2:
   W2_i = W1_i iff word W1_i is in vocabulary B
          0    else
4) Map W2 to the semantic subspace to create the new basis vector ui'
5) The set of ui' constitute a new basis for b, and define a mapping:
         a -> b



Measures for convergence:
1) distance (difference, interpretation of the word cloud / vector) of:
    || e' - e || -> 0,
    e' = G_1( F_1(e) ),   e c A



Construct more interpertable semantic space:

use the freedom the arbitrary orientation of the embedded space gives us, to rotate
the basis such that the maximum number of elements of the basis in vocabulary space is zero.

start with seed word S
get vector v_0
get word with inverse
set middle 50% of vector to zero (keep max negative and positive values)

def setZero(W):
    d = 40
    c0 = np.percentile(W, d)
    c1 = np.percentile(W, 100 - d)
    Wp = np.where(W < c0, W, 0)
    Wp = np.where(W > c1, W, Wp)
    Wp = Wp / np.linalg.norm(Wp)
    return Wp

def show(v):
    W = V2W_A(v)
    W.sort()
    W = W[::-1]
    plt.plot(W)
    plt.show()

def loop(v):
    W = V2W_A(v)
    W = setZero(W)
    v = W2V_A(W)
    return v

def pp(v):
    W = V2W_A(v)
    printVocabTopW(A, W, 10)
    remaining = np.sum( np.where( W > 0.005, 1, 0) )
    print "Elements larger than 0.005: ", remaining

Wmens = setZero( V2W_A( vmens ) )
Wmens /= np.linalg.norm(Wmens)

while True:
    # constrain
    W = V2W_A( v )
    W = setZero( W )

    # orthogonalize
    print "Dot product: ", np.dot(W, Wmens)
    W -= 0.01 * np.dot(W, Wmens)

    # re normalize
    W = W / np.linalg.norm(W)

    # get vector
    v = W2V_A(W)
    pp(v)

seed = 'boek'
v = A[seed]
for i in range(1000):
   v = loop(v)


W2V( W1 ) -> v1
V2W( v1 ) = W2 -> W1 + e1

W2V( W1 + e1 ) = W2V(W1) + W2V(e1) = v1 + W2V(e1)
W2V( W2 - W1 ) = W2V(W2) - W2V(W1)
               = v1 - v1 = 0

OBSERVATION:
    noise and OCR mistakes seem to cluster: printVocabTopW(A, V2W_A( A['dc7xd6'] ), 1000)

    investigate if we can get a proper rotation having:  O x O.transpose = 1
